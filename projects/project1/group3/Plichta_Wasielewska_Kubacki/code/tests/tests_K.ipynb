{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**IWLS Code**"
      ],
      "metadata": {
        "id": "FvwQyju-sN5f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iafVjU0NNnDT"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import time\n",
        "\n",
        "def check_data(X,y):\n",
        "  if len(X) != len(y):\n",
        "    return False, \"Length of the answer vector doesn't fit the number of data points.\\n\"\n",
        "  for i in range(1,len(X)):\n",
        "    if len(X[i]) != len(X[0]):\n",
        "      return False, \"The row at the index \"+str(i)+\" seems to be missing an observation.\\n\"\n",
        "  for i in range(len(y)):\n",
        "    if y[i] < 0 or y[i] > 1:\n",
        "      return False, \"The answer at the index \"+str(i)+\" doesn't indicate a binary class nor does it indicate a probability of belonging to one.\\n\"\n",
        "  return True, \"\"\n",
        "\n",
        "def calc_loglike(X, Y, beta):\n",
        "  \"\"\"\n",
        "  Calculates the value of the log-likelihood for a given training data,\n",
        "  answer vector and model parameters\n",
        "  :param X: a 2d numpy array containing the experiment matrix.\n",
        "  :param Y: a 1d numpy array containing the answer vector.\n",
        "  :param beta: a 1d numpy array containing the parameters of the model.\n",
        "  :return: a single float with the value of the log-likelihood function.\n",
        "  \"\"\"\n",
        "  Xbeta = X @ beta\n",
        "  return Y @ Xbeta - np.sum(np.log(1+np.exp(Xbeta)))\n",
        "\n",
        "def fit_with_IWLS(data, answers, intercept: bool = True,\n",
        "                  relevant_variables = None, additional_interactions = None,\n",
        "                  l2_reg: float = 1.0, beta0_gen = None,\n",
        "                  max_iterations: int = 500, min_step_norm: float = 1e-4,\n",
        "                  max_time: float = 3600.0, check_data: bool = False):\n",
        "  \"\"\"\n",
        "  Calculates the coefficients of a Logistic Regression model using Iterative\n",
        "  Weighted Least Squares method.\n",
        "  :param data: The data on which the model will be fit.\n",
        "  :param answers: The vector with answers (numbers belonging to the\n",
        "    [0,1] interval).\n",
        "  :param intercept: If True, the model will fit an intercept (meaning beta' @ x\n",
        "    shall be replaced with beta' @ x + beta0 in all calculations).\n",
        "  :param relevant_variables: A collection of indices, indicating on which\n",
        "    columns of the data should the model be built.\n",
        "    If None, all columns will be used.\n",
        "  :param additional_interactions: A collection of pairs of indices, indicating\n",
        "    which column element-wise products should be using for building the model.\n",
        "    If None, no such variables will be considered.\n",
        "  :param l2_reg: The strength of ridge regularization (the coefficient of\n",
        "    the ridge penalty). 0 means no regularization. 1 is the default, same as\n",
        "    in the scikit-learn implementation.\n",
        "  :param beta0_gen: A generator used to determine the starting values of\n",
        "    coefficients. Should include .generate(n: int) method, returning a numpy\n",
        "    array of length n filled with floats.\n",
        "    If None, all coefficients will be initialized to zeros.\n",
        "  :param max_iterations: The maximum number of iterations the algorhithm will\n",
        "    perform before stopping and proposing a solution.\n",
        "    By default, 100 in accordance to scikit-learn implementation.\n",
        "  :param min_step_norm: The minimum value for the euclidian norm of the change\n",
        "    of a parameter vector in a single step. If the difference between\n",
        "    iterations falls below that number, the algorhithm will stop and propose\n",
        "    a solution.\n",
        "  :param max_time: The maximum time the procedure can run in seconds. Once\n",
        "    exceeded, the iterating will stop and the solution will be proposed.\n",
        "  :param check_data: If True, the format of data and answers will be examined\n",
        "    prior to running the algorhithm.\n",
        "\n",
        "  :return: A numpy array containing the proposed coefficients and a dictionary,\n",
        "    labeling said coefficients, a list containing the coefficients after each\n",
        "    iteration and a list containing the values of the log-likelihood function\n",
        "    after each iteration.\n",
        "  \"\"\"\n",
        "  ### Ensuring the correct dimensionality of the data.\n",
        "  if check_data:\n",
        "    status, message = check_data(data, answers)\n",
        "    assert status, message\n",
        "  assert len(data) == len(answers), \"For every data point, there has to be a correct class specified.\\n\"\n",
        "  n = len(data)\n",
        "\n",
        "  ### Filling up the default values of parameters.\n",
        "  if additional_interactions is None:\n",
        "    additional_interactions = []\n",
        "  if relevant_variables is None:\n",
        "    relevant_variables = np.arange(len(data[0]))\n",
        "\n",
        "  ### Constructing the experiment matrix and labels for it.\n",
        "  Y = np.array(answers)\n",
        "  X = []\n",
        "  labels = []\n",
        "  for index in relevant_variables:\n",
        "    X.append(np.array(data[:,index]).astype(float))\n",
        "    labels.append(\"X\"+str(index))\n",
        "\n",
        "  for index1, index2 in additional_interactions:\n",
        "    X.append(np.array(data[:,index1]).astype(float) * np.array(data[:,index2]).astype(float))\n",
        "    labels.append(\"X\"+str(index1)+\"X\"+str(index2))\n",
        "\n",
        "  if intercept:\n",
        "    X.append(np.ones(n))\n",
        "    labels.append(\"intercept\")\n",
        "\n",
        "  X = np.column_stack(X)\n",
        "  p = len(labels)\n",
        "  # If the penalty is the l2_reg times the sum of squares of coefficients, this\n",
        "  # is the matrix of the second order derivatives with respect to the coefs.\n",
        "  penalty_hessian = 2 * l2_reg * np.eye(p)\n",
        "\n",
        "  ### Initializing coefficients.\n",
        "  if beta0_gen is None:\n",
        "    beta = np.zeros(p)\n",
        "  else:\n",
        "    beta = beta0_gen.generate(p)\n",
        "\n",
        "  start = time.time()\n",
        "  beta_hist = []\n",
        "  loglike_hist = []\n",
        "  beta_hist.append(beta)\n",
        "  loglike_hist.append(calc_loglike(X, Y, beta))\n",
        "  ### Iterating the main algorhithm.\n",
        "  for _ in range(max_iterations):\n",
        "    P = X @ beta\n",
        "    P = np.exp(P) / (np.exp(P) + 1)\n",
        "    W = np.diag(P * (1 - P))\n",
        "\n",
        "    # deriv is the derivative of the minus log-like + penalty with respect to beta\n",
        "    deriv =  X.transpose() @ (P-Y) + 2 * l2_reg * beta\n",
        "    # hessian is the matrix of second order derivatives of the previously mentioned function\n",
        "    hessian = X.transpose() @ W @ X + penalty_hessian\n",
        "\n",
        "    # In order to avoid numerical complexity of the matrix inversion,\n",
        "    # new beta is defined as a solution to a linear equation.\n",
        "    beta_new = np.linalg.solve(hessian, hessian @ beta - deriv)\n",
        "\n",
        "    diff = beta - beta_new\n",
        "    diff_norm = np.sqrt(diff @ diff)\n",
        "    beta = beta_new\n",
        "\n",
        "    beta_hist.append(beta)\n",
        "    loglike_hist.append(calc_loglike(X, Y, beta))\n",
        "\n",
        "    if diff_norm < min_step_norm:\n",
        "      break\n",
        "    curr = time.time()\n",
        "    if curr - start > max_time:\n",
        "      break\n",
        "\n",
        "  ### Creating the coefficient dictionary.\n",
        "  beta_dict = {}\n",
        "  for i in range(p):\n",
        "    beta_dict[labels[i]] = beta[i]\n",
        "\n",
        "  return beta, beta_dict, beta_hist, loglike_hist\n",
        "\n",
        "def predict_proba(data, beta, intercept: bool = True, relevant_variables = None,\n",
        "                  additional_interactions = None):\n",
        "  \"\"\"\n",
        "  Predicts the probabilities of a given data points belonging to class 1,\n",
        "  assuming that the parameters of the model are known.\n",
        "  :param data: the datapoints the class of which you want to predict.\n",
        "  :param beta: a numpy array containing the model parameters in order: column\n",
        "    coefficients, product coefficients (if any additional interactions were\n",
        "    involved), intercept (if it was included).\n",
        "  :param intercept: choose True if model has been built with intercept in mind.\n",
        "  :param relevant_variables: A list of indices of columns on which the model\n",
        "    was built. If None, all columns are considered.\n",
        "  :param additional_interactions: A collection of pairs of indices, indicating\n",
        "    which column element-wise products were used for building the model.\n",
        "    If None, no such variables will be considered.\n",
        "  :return: a numpy array containing the probability of belonging to class 1\n",
        "    for each observation.\n",
        "  \"\"\"\n",
        "  ### Filling up the default values of parameters.\n",
        "  if additional_interactions is None:\n",
        "    additional_interactions = []\n",
        "  if relevant_variables is None:\n",
        "    relevant_variables = np.arange(len(data[0]))\n",
        "\n",
        "  ### Constructing the data matrix.\n",
        "  n = len(data)\n",
        "  X = []\n",
        "  for index in relevant_variables:\n",
        "    X.append(np.array(data[:,index]).astype(float))\n",
        "\n",
        "  for index1, index2 in additional_interactions:\n",
        "    X.append(np.array(data[:,index1]).astype(float) * np.array(data[:,index2]).astype(float))\n",
        "\n",
        "  if intercept:\n",
        "    X.append(np.ones(n))\n",
        "\n",
        "  X = np.column_stack(X)\n",
        "\n",
        "  exp = np.exp(X @ beta)\n",
        "  return exp/(1+exp)\n",
        "\n",
        "def predict(data, beta, intercept: bool = True, relevant_variables = None,\n",
        "            additional_interactions = None, threshold: float = 0.5):\n",
        "  \"\"\"\n",
        "  A classification based on the 'predict_proba' method output.\n",
        "  \"\"\"\n",
        "  probas = predict_proba(data, beta, intercept=intercept,\n",
        "                         relevant_variables=relevant_variables,\n",
        "                         additional_interactions=additional_interactions)\n",
        "  return (probas > threshold).astype(int)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Datasets**"
      ],
      "metadata": {
        "id": "TRExWta6xZSB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96lQLSyrz2Su",
        "outputId": "a416b8c0-79b8-4b53-c676-e7b0fa0999e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ucimlrepo\n",
            "  Downloading ucimlrepo-0.0.6-py3-none-any.whl (8.0 kB)\n",
            "Installing collected packages: ucimlrepo\n",
            "Successfully installed ucimlrepo-0.0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo\n",
        "import pandas as pd\n",
        "import heapq\n",
        "from sklearn.linear_model import LinearRegression"
      ],
      "metadata": {
        "id": "8Mgdfx0IzcVe"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_collinearity(columns, indices, threshold = 1e-10):\n",
        "  \"\"\"\n",
        "  Helper function, testing whether a certain subset of columns is collinear.\n",
        "  :param columns: the whole set of columns.\n",
        "  :param indices: indices belonging to the subset.\n",
        "  :param threshold: the value of determinant, going below which will be\n",
        "    considered being numerically collinear.\n",
        "  :return: True if collinear, False otherwise.\n",
        "  \"\"\"\n",
        "  used_columns = []\n",
        "  for index in indices:\n",
        "    used_columns.append(columns[index])\n",
        "\n",
        "  X = np.column_stack(used_columns)\n",
        "  XX = X.transpose() @ X\n",
        "  if np.linalg.det(XX) < threshold:\n",
        "    return True\n",
        "  else:\n",
        "    return False\n",
        "\n",
        "def remove_collinear(X):\n",
        "  \"\"\"\n",
        "  Removes the minimum number of columns to ensure the result matrix will be\n",
        "  full rank.\n",
        "  :param X: a numpy matrix one needs a non-collinear version of.\n",
        "  :return: a numpy matrix with collinearities removed and a set containing\n",
        "    indices of removed columns.\n",
        "  \"\"\"\n",
        "  columns = []\n",
        "  p = len(X[0])\n",
        "  for i in range(p):\n",
        "    columns.append(X[:,i])\n",
        "\n",
        "  columns_used = []\n",
        "  columns_stashed = set()\n",
        "  columns_removed = set()\n",
        "  for i in range(p):\n",
        "    columns_used.append(i)\n",
        "\n",
        "  heapq.heapify(columns_used)\n",
        "\n",
        "  last_removed = -1\n",
        "  while(True):\n",
        "    if len(columns_used) == 0:\n",
        "      break\n",
        "\n",
        "    if test_collinearity(columns, columns_used):\n",
        "      last_removed = heapq.heappop(columns_used)\n",
        "      columns_stashed.add(last_removed)\n",
        "    else:\n",
        "      if last_removed == -1:\n",
        "        # If the whole remaining subset is non-collinear, it's time to stop.\n",
        "\n",
        "        break\n",
        "      else:\n",
        "        # If removing a certain column made the subset non-collinear, it means\n",
        "        # that this column is a good candidate for removal.\n",
        "\n",
        "        columns_stashed.remove(last_removed)\n",
        "        columns_removed.add(last_removed)\n",
        "\n",
        "        # Returning stashed away columns back to the subset.\n",
        "        for index in columns_stashed:\n",
        "          columns_used.append(index)\n",
        "        heapq.heapify(columns_used)\n",
        "        columns_stashed.clear()\n",
        "        last_removed = -1\n",
        "\n",
        "  # Recreating the matrix\n",
        "  is_used = [False for i in range(p)]\n",
        "  for index in columns_used:\n",
        "    is_used[index] = True\n",
        "\n",
        "  columns_used = []\n",
        "  for i in range(p):\n",
        "    if is_used[i]:\n",
        "      columns_used.append(columns[i])\n",
        "\n",
        "  X_clean = np.column_stack(columns_used)\n",
        "  return X_clean, columns_removed\n"
      ],
      "metadata": {
        "id": "rp-TrDv2xcav"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_heart_disease():\n",
        "  # fetch dataset\n",
        "  data_heart_disease = fetch_ucirepo(id=45)\n",
        "\n",
        "  # data (as pandas dataframes)\n",
        "  X = data_heart_disease.data.features\n",
        "  y = data_heart_disease.data.targets\n",
        "\n",
        "  # removing missing variables\n",
        "  X = X.dropna()\n",
        "  y = y.loc[X.index]\n",
        "\n",
        "  # one-hot-encoding the detected multi-valued categorical variables\n",
        "  X = pd.get_dummies(X, columns=['cp', 'restecg', 'slope', 'thal'],\n",
        "                     drop_first=True, dtype=int)\n",
        "\n",
        "  # changing the format to numpy arrays\n",
        "  # (flattening is necessary for y as .values isn't smart enough to notice that\n",
        "  # y had only one column)\n",
        "  X = X.values\n",
        "  y = y.values.flatten()\n",
        "\n",
        "  # mapping the answers to {0,1}\n",
        "  y = (y==0)\n",
        "  y = y.astype(int)\n",
        "\n",
        "  # removing collinearities\n",
        "  X, _ = remove_collinear(X)\n",
        "\n",
        "  return X, y\n",
        "\n",
        "def fetch_parkinsons():\n",
        "  # fetch dataset\n",
        "  data_parkinsons = fetch_ucirepo(id=174)\n",
        "\n",
        "  # data (as pandas dataframes)\n",
        "  X = data_parkinsons.data.features\n",
        "  y = data_parkinsons.data.targets\n",
        "\n",
        "  # changing the format to numpy arrays\n",
        "  # (flattening is necessary for y as .values isn't smart enough to notice that\n",
        "  # y had only one column)\n",
        "  X = X.values\n",
        "  y = y.values.flatten()\n",
        "\n",
        "  # removing collinearities\n",
        "  X, _ = remove_collinear(X)\n",
        "\n",
        "  return X, y\n",
        "\n",
        "def fetch_hcv():\n",
        "  # fetch dataset\n",
        "  data_hcv = fetch_ucirepo(id=571)\n",
        "\n",
        "  # data (as pandas dataframes)\n",
        "  X = data_hcv.data.features\n",
        "  y = data_hcv.data.targets\n",
        "\n",
        "  # mapping the 'Sex' column to numeric values\n",
        "  X.loc[:,'Sex'] = X['Sex'].map({'m': 0, 'f': 1})\n",
        "  # mapping the target variable to {0,1}\n",
        "  y.loc[:,'Category'] = y['Category'].map({'0=Blood Donor': 0,\n",
        "                                           '0s=suspect Blood Donor': 0,\n",
        "                                           '1=Hepatitis': 1, '2=Fibrosis': 1,\n",
        "                                           '3=Cirrhosis': 1})\n",
        "\n",
        "  # removing rare missing values\n",
        "  X = X.dropna(subset=['ALB','PROT','ALT'])\n",
        "  y = y.loc[X.index]\n",
        "\n",
        "  # Regressing for the remaining missing values\n",
        "  X_for_lr = X.dropna()\n",
        "  y_for_lr1 = X_for_lr['ALP'].values\n",
        "  y_for_lr2 = X_for_lr['CHOL'].values\n",
        "  X_for_lr = X_for_lr.drop(columns=['ALP','CHOL']).values\n",
        "\n",
        "  lr = LinearRegression()\n",
        "\n",
        "  lr.fit(X_for_lr, y_for_lr1)\n",
        "  ALP_missing = X[X['ALP'].isna()]\n",
        "  data_for_ALP_predicting = ALP_missing.drop(columns=['ALP', 'CHOL']).values\n",
        "  ALP_predictions = lr.predict(data_for_ALP_predicting)\n",
        "  X.loc[X['ALP'].isna(), 'ALP'] = ALP_predictions\n",
        "\n",
        "  lr.fit(X_for_lr, y_for_lr2)\n",
        "  CHOL_missing = X[X['CHOL'].isna()]\n",
        "  data_for_CHOL_predicting = CHOL_missing.drop(columns=['ALP', 'CHOL']).values\n",
        "  CHOL_predictions = lr.predict(data_for_CHOL_predicting)\n",
        "  X.loc[X['CHOL'].isna(), 'CHOL'] = CHOL_predictions\n",
        "\n",
        "  # changing the format to numpy arrays\n",
        "  # (flattening is necessary for y as .values isn't smart enough to notice that\n",
        "  # y had only one column)\n",
        "  X = X.values\n",
        "  y = y.values.flatten()\n",
        "\n",
        "  # removing collinearities\n",
        "  X, _ = remove_collinear(X)\n",
        "\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "PR7z7W5BzgEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "def fetch_sonar():\n",
        "  # fetch dataset\n",
        "  connectionist_bench_sonar_mines_vs_rocks = fetch_ucirepo(id=151)\n",
        "\n",
        "  # data (as pandas dataframes)\n",
        "  X = connectionist_bench_sonar_mines_vs_rocks.data.features\n",
        "  y = connectionist_bench_sonar_mines_vs_rocks.data.targets\n",
        "\n",
        "  #Mapping target classes R - rock to 0, M - mine to 1\n",
        "  y['class'] = y['class'].replace({'R': 0, 'M': 1})\n",
        "  target_counts = y['class'].value_counts()\n",
        "\n",
        "  X = X.values\n",
        "  y = np.asarray(y)\n",
        "  y = y.flatten()\n",
        "  return X, y\n",
        "\n",
        "def fetch_breast_cancer():\n",
        "  data = load_breast_cancer()\n",
        "  X = data['data']\n",
        "  y = data.target\n",
        "\n",
        "  X, _ = remove_collinear(X)\n",
        "  return X, y\n",
        "\n",
        "def fetch_ionsphere():\n",
        "  # fetch dataset\n",
        "  ionosphere = fetch_ucirepo(id=52)\n",
        "\n",
        "  # data (as pandas dataframes)\n",
        "  X = ionosphere.data.features\n",
        "  y = ionosphere.data.targets\n",
        "\n",
        "  #Mapping target classes b - bad to 0, g - good to 1\n",
        "  y['Class'] = y['Class'].replace({'b': 0, 'g': 1})\n",
        "\n",
        "  X = X.values\n",
        "  y = np.asarray(y)\n",
        "  y = y.flatten()\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "kWNEsyLZ18K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def fetch_diabetes():\n",
        "  df = pd.read_csv(\"diabetes.csv\")\n",
        "  X = df.values\n",
        "  y = X[:,-1]\n",
        "  X = X[:,:-1]\n",
        "\n",
        "  X, _ = remove_collinear(X)\n",
        "  return X, y\n",
        "\n",
        "def fetch_fraud_detection():\n",
        "  df = pd.read_csv(\"fraud_detection_dataset.csv\")\n",
        "  X = df[['amount', 'age', 'income', 'debt', 'credit_score']]\n",
        "  y = df['is_fraud']\n",
        "\n",
        "  X = X.values\n",
        "  y = y.values.flatten()\n",
        "\n",
        "  X, _ = remove_collinear(X)\n",
        "  return X, y\n",
        "\n",
        "def fetch_banknote_authentication():\n",
        "  df = pd.read_csv(\"data_banknote_authentication.txt\", header=None)\n",
        "\n",
        "  X = df.values\n",
        "  y = X[:,-1]\n",
        "  X = X[:,:-1]\n",
        "\n",
        "  X, _ = remove_collinear(X)\n",
        "  return X, y"
      ],
      "metadata": {
        "id": "rw_Ee_4uE0Vu"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing functions**"
      ],
      "metadata": {
        "id": "78tNhBUYOf4_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import balanced_accuracy_score\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "sDFXYElVOjBn"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_interactions(n: int):\n",
        "  result = []\n",
        "  for i in range(n):\n",
        "    for j in range(i+1,n):\n",
        "      result.append((i,j))\n",
        "  return result"
      ],
      "metadata": {
        "id": "_6PU_3O4S5rp"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_with_cv(X, y, count=5, include_interactions=False):\n",
        "  batches_X = [[] for i in range(count)]\n",
        "  batches_y = [[] for i in range(count)]\n",
        "  for i in range(len(y)):\n",
        "    batches_X[i % count].append(X[i])\n",
        "    batches_y[i % count].append(y[i])\n",
        "\n",
        "  interactions = None\n",
        "  if include_interactions:\n",
        "    interactions = generate_interactions(len(X[0]))\n",
        "\n",
        "  results = {}\n",
        "  results['acc'] = []\n",
        "  results['beta_hists'] = []\n",
        "  results['loglike_hists'] = []\n",
        "  for k in range(count):\n",
        "    X_train = []\n",
        "    y_train = []\n",
        "    for j in range(k+1, k+count):\n",
        "      index = j % count\n",
        "      for i in range(len(batches_y[index])):\n",
        "        X_train.append(batches_X[index][i])\n",
        "        y_train.append(batches_y[index][i])\n",
        "\n",
        "    X_train = np.row_stack(X_train)\n",
        "    y_train = np.array(y_train)\n",
        "\n",
        "    X_test = np.row_stack(batches_X[k])\n",
        "    y_test = np.array(batches_y[k])\n",
        "\n",
        "\n",
        "    beta, _, beta_hist, loglike_hist = fit_with_IWLS(X_train, y_train, intercept=True, additional_interactions=interactions)\n",
        "\n",
        "    results['beta_hists'].append(beta_hist)\n",
        "    results['loglike_hists'].append(loglike_hist)\n",
        "\n",
        "    y_preds = predict(X_test, beta, intercept=True, additional_interactions=interactions)\n",
        "    results['acc'].append(balanced_accuracy_score(y_test, y_preds))\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "dyxazjcnO-cS"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_avg_loglike(results):\n",
        "  min_length = 500\n",
        "  for history in results['loglike_hists']:\n",
        "    if len(history) < min_length:\n",
        "      min_length = len(history)\n",
        "\n",
        "  result = [0 for i in range(min_length)]\n",
        "  for i in range(min_length):\n",
        "    for history in results['loglike_hists']:\n",
        "      result[i] += history[i]\n",
        "    result[i] /= len(results['loglike_hists'])\n",
        "  return result"
      ],
      "metadata": {
        "id": "iY-1f5LMlPID"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Testing**"
      ],
      "metadata": {
        "id": "fG9BEG5KUIaC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_heart_disease()\n",
        "results = test_with_cv(X, y)"
      ],
      "metadata": {
        "id": "TyZaGYUGUK04"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"acc: \", results['acc'])\n",
        "print(\"avg acc: \", np.mean(results['acc']))\n",
        "print(\"avg log-like: \", calc_avg_loglike(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkeIygy7ihVm",
        "outputId": "bc9c8f56-0996-47a8-b79a-0d1b0b704169"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  [0.8665183537263625, 0.7767857142857143, 0.846551724137931, 0.9288194444444444, 0.7827380952380952]\n",
            "avg acc:  0.8402826663665095\n",
            "avg-log-like:  [-164.691770101043, -90.87104779347564, -80.85068442189352, -78.90350565105332, -78.70219934604549, -78.6973602395561, -78.69735526662458]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_parkinsons()\n",
        "results = test_with_cv(X, y)"
      ],
      "metadata": {
        "id": "XhYdjapylA-e"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"acc: \", results['acc'])\n",
        "print(\"avg acc: \", np.mean(results['acc']))\n",
        "print(\"avg log-like: \", calc_avg_loglike(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO5c9Z0VlCDj",
        "outputId": "7bd5b4b0-4889-4780-b3dd-2c2187ad1b9e"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  [0.7272727272727273, 0.6888888888888889, 0.8333333333333333, 0.6827586206896552, 0.7611111111111111]\n",
            "avg acc:  0.7386729362591431\n",
            "avg log-like:  [-108.13096016735149, -61.364635149873436, -54.433155656471875, -52.92259525667531, -52.81262995626339, -52.811891772692114]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_hcv()\n",
        "results = test_with_cv(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ldkGvT53nAeu",
        "outputId": "84fb7ebf-4a49-4696-ed1d-11c37d404a71"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-e1f82b089fd5>:60: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  X.loc[:,'Sex'] = X['Sex'].map({'m': 0, 'f': 1})\n",
            "<ipython-input-8-e1f82b089fd5>:60: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  X.loc[:,'Sex'] = X['Sex'].map({'m': 0, 'f': 1})\n",
            "<ipython-input-8-e1f82b089fd5>:62: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  y.loc[:,'Category'] = y['Category'].map({'0=Blood Donor': 0,\n",
            "<ipython-input-8-e1f82b089fd5>:62: DeprecationWarning: In a future version, `df.iloc[:, i] = newvals` will attempt to set the values inplace instead of always setting a new array. To retain the old behavior, use either `df[df.columns[i]] = newvals` or, if columns are non-unique, `df.isetitem(i, newvals)`\n",
            "  y.loc[:,'Category'] = y['Category'].map({'0=Blood Donor': 0,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"acc: \", results['acc'])\n",
        "print(\"avg acc: \", np.mean(results['acc']))\n",
        "print(\"avg log-like: \", calc_avg_loglike(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kVDNW8hnF7T",
        "outputId": "f2dac256-92a2-4a36-ab5a-34e9718f1f30"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  [0.8287037037037037, 0.9287037037037037, 0.9239417989417988, 0.7764550264550265, 0.8121693121693122]\n",
            "avg acc:  0.853994708994709\n",
            "avg log-like:  [-339.36485960214924, -116.48502158269687, -78.5616628012908, -64.00660117348124, -58.802705602141394, -57.728210296601176, -57.64189004251489, -57.63915586902315, -57.6391402058686]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_sonar()\n",
        "results = test_with_cv(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c5gkda2dnOrP",
        "outputId": "61ff5a12-cc13-4fbe-8595-deb5be965bcf"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-1aecae88fd00>:12: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  y['class'] = y['class'].replace({'R': 0, 'M': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"acc: \", results['acc'])\n",
        "print(\"avg acc: \", np.mean(results['acc']))\n",
        "print(\"avg log-like: \", calc_avg_loglike(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yuCEjTA4nXLq",
        "outputId": "1d2f1f58-eb9e-4fd5-88e4-e646b661dd40"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  [0.8772727272727273, 0.8772727272727273, 0.7242562929061784, 0.7248803827751196, 0.6949760765550239]\n",
            "avg acc:  0.7797316413563553\n",
            "avg log-like:  [-115.33969084517491, -83.96401543701369, -81.89783083413846, -81.83692015062695, -81.8368518968259]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_breast_cancer()\n",
        "results = test_with_cv(X, y)"
      ],
      "metadata": {
        "id": "S4NBW2DmndyM"
      },
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"acc: \", results['acc'])\n",
        "print(\"avg acc: \", np.mean(results['acc']))\n",
        "print(\"avg log-like: \", calc_avg_loglike(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eARA42B7nlh7",
        "outputId": "b0089d58-89af-481a-cfa1-14e38506343d"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  [0.922972972972973, 0.9144736842105263, 0.974375, 0.9315476190476191, 0.9404761904761905]\n",
            "avg acc:  0.9367690933414619\n",
            "avg log-like:  [-315.52059659088707, -129.92649976260924, -88.71729988525442, -66.78235058622298, -52.83391369037097, -46.70816255411982, -45.25572876124456, -45.06471234608607, -45.05538263220019, -45.05534573241543]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_ionsphere()\n",
        "results = test_with_cv(X, y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E49xJhqRny68",
        "outputId": "33e36c67-805f-4a08-a69a-9db66ced1031"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-13-1aecae88fd00>:37: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  y['Class'] = y['Class'].replace({'b': 0, 'g': 1})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"acc: \", results['acc'])\n",
        "print(\"avg acc: \", np.mean(results['acc']))\n",
        "print(\"avg log-like: \", calc_avg_loglike(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ei6FyAvfn4sD",
        "outputId": "fe4fdf47-65d9-43a6-e471-229aa8d294f2"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  [0.7743589743589744, 0.8088888888888889, 0.88, 0.8347902097902098, 0.75]\n",
            "avg acc:  0.8096076146076147\n",
            "avg log-like:  [-194.63572830123266, -94.58296980448672, -79.53875600116562, -76.81387647497023, -76.64295466219465, -76.64151419978084, -76.64151390763423]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_diabetes()\n",
        "results = test_with_cv(X, y)"
      ],
      "metadata": {
        "id": "ijqx6HaroAqU"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"acc: \", results['acc'])\n",
        "print(\"avg acc: \", np.mean(results['acc']))\n",
        "print(\"avg log-like: \", calc_avg_loglike(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TGQygazcoHKl",
        "outputId": "6920e340-9a52-4747-b4a0-15853f12aae3"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  [0.7772988505747127, 0.7002551020408163, 0.6770833333333334, 0.7095392231530846, 0.646236559139785]\n",
            "avg acc:  0.7020826136483465\n",
            "avg log-like:  [-425.86962773603045, -312.21558454230745, -304.75721366088936, -304.42429951200586, -304.4225206774464]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_diabetes()\n",
        "results = test_with_cv(X, y, include_interactions=True)"
      ],
      "metadata": {
        "id": "d0zS0c4doImb"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"acc: \", results['acc'])\n",
        "print(\"avg acc: \", np.mean(results['acc']))\n",
        "print(\"avg log-like: \", calc_avg_loglike(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v90tyzy7ogZz",
        "outputId": "e42fc14c-08a2-4e19-9508-abc8cc6508f2"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  [0.7564655172413793, 0.721938775510204, 0.7529761904761905, 0.6853579588728104, 0.6771505376344086]\n",
            "avg acc:  0.7187777959469985\n",
            "avg log-like:  [-425.86962773603045, -286.9877443055924, -272.70018214198075, -270.94175795302266, -270.87295644205676, -270.8721976942828]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# No RAM for this\n",
        "X, y = fetch_fraud_detection()\n",
        "results = test_with_cv(X, y)"
      ],
      "metadata": {
        "id": "JkhBOENtol4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_banknote_authentication()\n",
        "results = test_with_cv(X, y)"
      ],
      "metadata": {
        "id": "W0x4u7yPpNi9"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"acc: \", results['acc'])\n",
        "print(\"avg acc: \", np.mean(results['acc']))\n",
        "print(\"avg log-like: \", calc_avg_loglike(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mtfqF_ibpf2O",
        "outputId": "504efdac-e4e0-4d04-a76a-59f0a86adcf7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  [0.9828297439194257, 0.9828297439194257, 0.9967105263157895, 0.9959016393442623, 0.9926121656600517]\n",
            "avg acc:  0.990176763831791\n",
            "avg log-like:  [-760.798345382596, -215.58025733631357, -111.44024578605612, -69.21652565770437, -49.60424019985176, -39.72051077098813, -35.374569015704495, -34.305654412693, -34.23834422174868, -34.238080498187266]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X, y = fetch_banknote_authentication()\n",
        "results = test_with_cv(X, y, include_interactions=True)"
      ],
      "metadata": {
        "id": "0Xn3xNxYpsM0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"acc: \", results['acc'])\n",
        "print(\"avg acc: \", np.mean(results['acc']))\n",
        "print(\"avg log-like: \", calc_avg_loglike(results))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6oXlYY1zpw4P",
        "outputId": "1494bc68-d9d2-427b-dc81-53fcf78d6f5f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc:  [1.0, 1.0, 1.0, 1.0, 1.0]\n",
            "avg acc:  1.0\n",
            "avg log-like:  [-760.798345382596, -192.29705942063072, -89.68717565005963, -47.51610743630963, -26.366779412160575, -16.114835293085797, -11.98423249518537, -10.48226496232719, -10.016626682581864, -9.94321156603337, -9.937434670455422, -9.93738756385228]\n"
          ]
        }
      ]
    }
  ]
}