{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from LogisticRegressors import SGD, ADAM, IWLS\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import balanced_accuracy_score as bal_acc\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import json\n",
    "import openml\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from utils import validate_datasets, preprocess_datasets, performance_test\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n",
    "warnings.simplefilter(action=\"ignore\", category=RuntimeWarning)\n",
    "\n",
    "import random\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: banknote-authentication\n",
      "Binary target - OK\n",
      "Low dimensionality (4) when assumed small (<=10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (1372) than columns (4) - OK\n",
      "Dataset banknote-authentication is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: puma8NH\n",
      "Binary target - OK\n",
      "Low dimensionality (8) when assumed small (<=10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (8192) than columns (8) - OK\n",
      "Dataset puma8NH is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: phoneme\n",
      "Binary target - OK\n",
      "Low dimensionality (5) when assumed small (<=10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (5404) than columns (5) - OK\n",
      "Dataset phoneme is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: bank32nh\n",
      "Binary target - OK\n",
      "High dimensionality (32) when assumed large (>10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (8192) than columns (32) - OK\n",
      "Dataset bank32nh is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: PizzaCutter1\n",
      "Binary target - OK\n",
      "High dimensionality (37) when assumed large (>10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (661) than columns (37) - OK\n",
      "Dataset PizzaCutter1 is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: MegaWatt1\n",
      "Binary target - OK\n",
      "High dimensionality (37) when assumed large (>10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (253) than columns (37) - OK\n",
      "Dataset MegaWatt1 is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: analcatdata_authorship\n",
      "Binary target - OK\n",
      "High dimensionality (70) when assumed large (>10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (841) than columns (70) - OK\n",
      "Dataset analcatdata_authorship is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: mc1\n",
      "Binary target - OK\n",
      "High dimensionality (38) when assumed large (>10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (9466) than columns (38) - OK\n",
      "Dataset mc1 is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: twonorm\n",
      "Binary target - OK\n",
      "High dimensionality (20) when assumed large (>10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (7400) than columns (20) - OK\n",
      "Dataset twonorm is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "All datasets are valid\n"
     ]
    }
   ],
   "source": [
    "with open(\"datasets.json\", \"r\") as file:\n",
    "    datasets = json.load(file)\n",
    "validate_datasets(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing dataset banknote-authentication\n",
      "Columns with only one unique value were dropped: set()\n",
      "Columns to drop: ['V3'] because of collinearity\n",
      "\n",
      "Preprocessing dataset puma8NH\n",
      "Columns with only one unique value were dropped: set()\n",
      "Columns to drop: [] because of collinearity\n",
      "\n",
      "Preprocessing dataset phoneme\n",
      "Columns with only one unique value were dropped: set()\n",
      "Columns to drop: [] because of collinearity\n",
      "\n",
      "Preprocessing dataset bank32nh\n",
      "Columns with only one unique value were dropped: set()\n",
      "Columns to drop: [] because of collinearity\n",
      "\n",
      "Preprocessing dataset PizzaCutter1\n",
      "Columns with only one unique value were dropped: set()\n",
      "Columns to drop: ['f', 'g', 'i', 'k', 'm', 'n', 'p', 's', 'u', 'v', 'z', 'ab', 'ac', 'ad', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'ao'] because of collinearity\n",
      "\n",
      "Preprocessing dataset MegaWatt1\n",
      "Columns with only one unique value were dropped: set()\n",
      "Columns to drop: ['f', 'g', 'i', 'k', 'm', 'n', 'p', 's', 't', 'u', 'v', 'z', 'aa', 'ab', 'ac', 'ae', 'af', 'ag', 'ah', 'ai', 'aj', 'ak', 'al', 'am', 'ao'] because of collinearity\n",
      "\n",
      "Preprocessing dataset analcatdata_authorship\n",
      "Columns with only one unique value were dropped: set()\n",
      "Columns to drop: ['had'] because of collinearity\n",
      "\n",
      "Preprocessing dataset mc1\n",
      "Columns with only one unique value were dropped: set()\n",
      "Columns to drop: ['LOC_COMMENTS', 'CONDITION_COUNT', 'CYCLOMATIC_COMPLEXITY', 'DECISION_COUNT', 'DESIGN_COMPLEXITY', 'EDGE_COUNT', 'LOC_EXECUTABLE', 'HALSTEAD_DIFFICULTY', 'HALSTEAD_EFFORT', 'HALSTEAD_ERROR_EST', 'HALSTEAD_LENGTH', 'HALSTEAD_PROG_TIME', 'HALSTEAD_VOLUME', 'MODIFIED_CONDITION_COUNT', 'MULTIPLE_CONDITION_COUNT', 'NODE_COUNT', 'NORMALIZED_CYLOMATIC_COMPLEXITY', 'NUM_OPERANDS', 'NUM_OPERATORS', 'NUM_UNIQUE_OPERANDS', 'NUM_UNIQUE_OPERATORS', 'NUMBER_OF_LINES', 'LOC_TOTAL'] because of collinearity\n",
      "\n",
      "Preprocessing dataset twonorm\n",
      "Columns with only one unique value were dropped: set()\n",
      "Columns to drop: [] because of collinearity\n",
      "\n"
     ]
    }
   ],
   "source": [
    "datasets = preprocess_datasets(datasets, collinear_threshold=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: banknote-authentication\n",
      "Binary target - OK\n",
      "Low dimensionality (3) when assumed small (<=10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (1372) than columns (3) - OK\n",
      "Dataset banknote-authentication is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: puma8NH\n",
      "Binary target - OK\n",
      "Low dimensionality (8) when assumed small (<=10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (8192) than columns (8) - OK\n",
      "Dataset puma8NH is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: phoneme\n",
      "Binary target - OK\n",
      "Low dimensionality (5) when assumed small (<=10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (5404) than columns (5) - OK\n",
      "Dataset phoneme is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: bank32nh\n",
      "Binary target - OK\n",
      "High dimensionality (32) when assumed large (>10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (8192) than columns (32) - OK\n",
      "Dataset bank32nh is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: PizzaCutter1\n",
      "Binary target - OK\n",
      "High dimensionality (13) when assumed large (>10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (661) than columns (13) - OK\n",
      "Dataset PizzaCutter1 is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: MegaWatt1\n",
      "Binary target - OK\n",
      "High dimensionality (12) when assumed large (>10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (253) than columns (12) - OK\n",
      "Dataset MegaWatt1 is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: analcatdata_authorship\n",
      "Binary target - OK\n",
      "High dimensionality (69) when assumed large (>10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (841) than columns (69) - OK\n",
      "Dataset analcatdata_authorship is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: mc1\n",
      "Binary target - OK\n",
      "High dimensionality (15) when assumed large (>10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (9466) than columns (15) - OK\n",
      "Dataset mc1 is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "Dataset: twonorm\n",
      "Binary target - OK\n",
      "High dimensionality (20) when assumed large (>10) - OK\n",
      "No missing values - OK\n",
      "No non-numeric columns - OK\n",
      "More rows (7400) than columns (20) - OK\n",
      "Dataset twonorm is OK\n",
      "----------------------------------------------------------------------------------------------------\n",
      "\n",
      "All datasets are valid\n"
     ]
    }
   ],
   "source": [
    "validate_datasets(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "reslut_dict = dict.fromkeys(datasets.keys(), dict())\n",
    "reslut_dict['MegaWatt1']= {\n",
    "    \"test\" : \"test\"\n",
    "}\n",
    "for key in reslut_dict.keys():\n",
    "    reslut_dict[key] = {\n",
    "        \"SGD\": dict(),\n",
    "        \"ADAM\": dict(),\n",
    "        \"IWLS\": dict(),\n",
    "    }\n",
    "reslut_dict_interactions = copy.deepcopy(reslut_dict)\n",
    "\n",
    "keys = list(reslut_dict_interactions.keys())\n",
    "\n",
    "# remove large datasets from reslut_dict_interactions\n",
    "for key in keys:\n",
    "    if datasets[key]['size'] == 'large':\n",
    "        reslut_dict_interactions.pop(key)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## All datasets no interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in tqdm(datasets.keys(), desc=\"Datasets\"):\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    X, y = datasets[dataset][\"X\"], datasets[dataset][\"y\"]\n",
    "    SGDs = [SGD(batch_size=1, epochs=500, intercept=True) for _ in range(5)]\n",
    "    ADAMs = [ADAM(batch_size=1, epochs=500, intercept=True) for _ in range(5)]\n",
    "    IWLSs = [IWLS(epochs=500, intercept=True) for _ in range(5)]\n",
    "    for model_name, models in tqdm(\n",
    "        {\n",
    "            \"SGD\": SGDs,\n",
    "            \"ADAM\": ADAMs,\n",
    "            \"IWLS\": IWLSs,\n",
    "        }.items(), desc=\"Models\"\n",
    "    ):\n",
    "        reslut_dict[dataset][model_name] = performance_test(X, y, models, model_name, test_size=0.2)\n",
    "    # print(reslut_dict)\n",
    "\n",
    "    with open(\"training_results.json\", \"w\") as file:\n",
    "        json.dump(reslut_dict, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Small datasets with interactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dataset in tqdm(datasets.keys(), desc=\"Datasets\"):\n",
    "    if datasets[dataset]['size'] == \"large\":\n",
    "        print(f\"Skipping {dataset} because it is large.\")\n",
    "        continue\n",
    "    print(f\"Dataset: {dataset}\")\n",
    "    X, y = datasets[dataset][\"X\"], datasets[dataset][\"y\"]\n",
    "    SGDs = [SGD(batch_size=1, epochs=500, intercept=True, include_interactions=True) for _ in range(5)]\n",
    "    ADAMs = [ADAM(batch_size=1, epochs=500, intercept=True, include_interactions=True) for _ in range(5)]\n",
    "    IWLSs = [IWLS(epochs=500, intercept=True, include_interactions=True) for _ in range(5)]\n",
    "    for model_name, models in tqdm(\n",
    "        {\n",
    "            \"SGD\": SGDs,\n",
    "            \"ADAM\": ADAMs,\n",
    "            \"IWLS\": IWLSs,\n",
    "        }.items(), desc=\"Models\"\n",
    "    ):\n",
    "        reslut_dict_interactions[dataset][model_name] = performance_test(X, y, models, model_name, test_size=0.2)\n",
    "\n",
    "    with open(\"training_results_interactions.json\", \"w\") as file:\n",
    "        json.dump(reslut_dict_interactions, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
